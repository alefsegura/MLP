{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    readDataset: Function that reads and randomize the dataset, returning a namedtuple -> dataset.X and dataset.Y\n",
    "\"\"\"\n",
    "def readDataset(filename, y_collumns):\n",
    "    \n",
    "    # Reading the dataset.\n",
    "    data = pd.read_csv(filename, index_col=False, header=None)\n",
    "    \n",
    "    # Acquiring dataset data and class data.\n",
    "    y = data.iloc[:,len(data.columns)-y_collumns: len(data.columns)]\n",
    "    y = np.array(y)\n",
    "    X = data.iloc[:,0:len(data.columns)-y_collumns]\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Randomizing dataset.\n",
    "    indices = np.random.choice(len(X), len(X), replace=False)\n",
    "    X_values = X[indices]\n",
    "    y_values = y[indices]\n",
    "    \n",
    "    # Creating an alias to dataset -> dataset.X and dataset.Y\n",
    "    dataset = namedtuple('datset', 'X Y')\n",
    "\n",
    "    return dataset(X=X_values, Y=y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    processing: Function that transform divides the dataset in train and test and transform the Y values in binary, OneHotEnconding\n",
    "\"\"\"\n",
    "def processing(dataset, p=0.75):\n",
    "    \n",
    "    # Normalizing process\n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(dataset.X)\n",
    "    #x = scaler.transform(dataset.X)\n",
    "    x = dataset.X\n",
    "    \n",
    "    # Labelizing process\n",
    "    #onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    #y = dataset.Y.reshape(len(dataset.Y), 1)\n",
    "    #y = onehot_encoder.fit_transform(y)\n",
    "    y = dataset.Y\n",
    "    \n",
    "    # Computing the lenght of dataset\n",
    "    lenght = dataset.X.shape[0]\n",
    "\n",
    "    # Split dataset into train and test\n",
    "    x_train = x[0:int(p*lenght), :]\n",
    "    y_train = y[0:int(p*lenght), :]\n",
    "\n",
    "    x_test = x[int(p*lenght):, :]\n",
    "    y_test = y[int(p*lenght):, :]\n",
    "        \n",
    "    # Creating an alias to train and test set\n",
    "    dataset = namedtuple('datset', 'X Y')\n",
    "    train = dataset(X=x_train, Y=y_train)\n",
    "    test = dataset(X=x_test, Y=y_test)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    sigmoid: Function that applies the sigmoid function, used in the backpropagation step.\n",
    "\"\"\" \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    mlp_forward: Function that is responsible for the forward step, that applies the actual weight values on the net.\n",
    "\"\"\"\n",
    "def mlp_forward(x, hidden_weights, output_weights):\n",
    "\n",
    "    f_net_h = []\n",
    "    # Apllying the weights on the hidden units\n",
    "    for i in range(len(hidden_weights)):\n",
    "        # If is the first hidden unit\n",
    "        if i == 0:\n",
    "            net = np.matmul(x,hidden_weights[i][:,0:len(x)].transpose()) + hidden_weights[i][:,-1]\n",
    "            f_net = sigmoid(net)\n",
    "        # If is the second or more hidden unit\n",
    "        else:\n",
    "            net = np.matmul(f_net_h[i-1],hidden_weights[i][:,0:len(f_net_h[i-1])].transpose()) + hidden_weights[i][:,-1]\n",
    "            f_net = sigmoid(net)\n",
    "        \n",
    "        # Store f_net of hidden layers\n",
    "        f_net_h.append(f_net) \n",
    "\n",
    "    # Computing the net function to the output layer\n",
    "    net = np.matmul(f_net_h[len(f_net_h)-1],output_weights[:,0:len(f_net_h[len(f_net_h)-1])].transpose()) + output_weights[:,-1]\n",
    "        \n",
    "    f_net_o = sigmoid(net)\n",
    "    \n",
    "    return f_net_o, f_net_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    mlp_backward: Function that is responsible for the backpropagation step, which corresponds to the updating of weights.\n",
    "\"\"\"\n",
    "def mlp_backward(dataset, j, hidden_weights, output_weights, f_net_o, f_net_h, eta, hidden_units, alpha, momentum_h, momentum_o, n_classes):\n",
    "\n",
    "    x = dataset.X[j,:]\n",
    "    y = dataset.Y[j,:]\n",
    "    \n",
    "    # Measuring the error\n",
    "    print(f_net_o)\n",
    "    print(y)\n",
    "    #error = y - f_net_o\n",
    "    \n",
    "    y_lista = int(''.join([str(l) for l in y]))\n",
    "    fneto_lista = int(''.join([str(l) for l in f_net_o]))    \n",
    "    error = y_lista - fneto_lista\n",
    "    \n",
    "    delta_o = error*f_net_o*(1-f_net_o)\n",
    "    \n",
    "    # Computing the delta for the hidden units\n",
    "    delta_h = []\n",
    "    for i in range(len(hidden_units)-1, -1, -1):\n",
    "\n",
    "        if(i == len(hidden_units)-1):\n",
    "            w_o = output_weights[: ,0:hidden_units[i]]\n",
    "            delta = (f_net_h[i]*(1-f_net_h[i]))*(np.matmul(delta_o, w_o))\n",
    "        else:\n",
    "            w_o = hidden_weights[i+1][:,0:hidden_units[i]]\n",
    "            delta = (f_net_h[i]*(1-f_net_h[i]))*(np.matmul(delta, w_o))\n",
    "\n",
    "        delta_h.insert(0,delta)\n",
    "    \n",
    "    # Computing the delta and updating weights for the output layer\n",
    "    delta_o = delta_o[:, np.newaxis]\n",
    "    f_net_aux = np.concatenate((f_net_h[len(hidden_units)-1],np.ones(1)))[np.newaxis, :]\n",
    "    output_weights = output_weights - -2*eta*np.matmul(delta_o, f_net_aux) + momentum_o\n",
    "    momentum_o = - -2*eta*np.matmul(delta_o, f_net_aux)\n",
    "    \n",
    "    # Updating the weights for the hidden layers\n",
    "    for i in range(len(hidden_units)-1, -1, -1):\n",
    "        delta = delta_h[i][:, np.newaxis]\n",
    "        f_net_aux = np.concatenate((f_net_h[i],np.ones(1)))[np.newaxis, :]    \n",
    "\n",
    "        if i == 0:\n",
    "            x_aux = np.concatenate((x,np.ones(1)))[np.newaxis, :]\n",
    "            hidden_weights[i] = hidden_weights[i] - -2*eta*np.matmul(delta, x_aux) + momentum_h[i]\n",
    "            momentum_h[i] = - -2*eta*np.matmul(delta, x_aux)\n",
    "        else:\n",
    "            f_net_aux = np.concatenate((f_net_h[i-1],np.ones(1)))[np.newaxis, :]\n",
    "            hidden_weights[i] = hidden_weights[i] - -2*eta*np.matmul(delta, f_net_aux) + momentum_h[i]\n",
    "            momentum_h[i] = - -2*eta*np.matmul(delta, f_net_aux)\n",
    "\n",
    "    #  Measuring the error\n",
    "    error = sum(error*error)\n",
    "\n",
    "    # Return the updated weights, the new error and the momentum parameters\n",
    "    return hidden_weights, output_weights, error, momentum_h, momentum_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    testing: Function that is responsible to realize the tests for the classification and regression methods\n",
    "             for different datasets.\n",
    "\"\"\"\n",
    "def testing(train, test, hidden_weights, output_weights):\n",
    "    counter = 0\n",
    "    for i in range(test.X.shape[0]):\n",
    "        y_hat, q = mlp_forward(test.X[i,:], hidden_weights, output_weights)\n",
    "        y_hat = np.argmax(y_hat)\n",
    "        y = np.argmax(test.Y[i,:])\n",
    "        if y == y_hat:\n",
    "            counter += 1\n",
    "            \n",
    "    return counter/test.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    MLP: function that is responsible to initialize weights, check conditions to construct net.\n",
    "\"\"\"\n",
    "def MLP(dataset, hidden_units, epochs, eta, alpha, train_size):\n",
    "    \n",
    "    # Acquiring the train and test set\n",
    "    train, test = processing(dataset, train_size)\n",
    "\n",
    "    n_classes = len(np.unique(dataset.X))\n",
    "    hidden_layers = len(hidden_units)\n",
    "    \n",
    "    # Initializing the weights of the hidden layers\n",
    "    momentum_o = 0\n",
    "    momentum_h = []\n",
    "    hidden_weights = []\n",
    "    for i in range(hidden_layers):\n",
    "        if(i == 0):\n",
    "            aux = np.zeros((hidden_units[i], dataset.X.shape[1] + 1))\n",
    "        else:\n",
    "            aux = np.zeros((hidden_units[i], hidden_units[i-1] + 1))\n",
    "    \n",
    "        hidden_weights.append(aux)\n",
    "        momentum_h.append(aux)\n",
    "\n",
    "    # Filling the hidden layers weight values with a normal distribution between -1 and 1\n",
    "    for i in range(hidden_layers):\n",
    "        for j in range(hidden_units[i]):\n",
    "            if(i == 0):\n",
    "                for k in range(dataset.X.shape[1] + 1):\n",
    "                    hidden_weights[i][j][k] = random.uniform(-1, 1)\n",
    "            else:\n",
    "                for k in range(hidden_units[i-1]+1):\n",
    "                    hidden_weights[i][j][k] = random.uniform(-1, 1)\n",
    "\n",
    "    # Initializing and filling the weights values of output layer\n",
    "    output_weights = np.zeros((n_classes, hidden_units[len(hidden_units)-1]+1))\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        for j in range(hidden_units[hidden_layers-1]+1):\n",
    "            output_weights[i][j] = random.uniform(-1, 1)\n",
    "\n",
    "    epoch = 0\n",
    "    for epoch in range(epochs):\n",
    "        sum_errors = 0\n",
    "        for i in range(train.X.shape[0]):\n",
    "            # Forward\n",
    "            f_net_o, f_net_h = mlp_forward(train.X[i, :], hidden_weights, output_weights)\n",
    "            # Backward hidden_weights, output_weights, error = \n",
    "            hidden_weights, output_weights, error, momentum_h, momentum_o = mlp_backward(train, i, hidden_weights, output_weights, \n",
    "                                                                                        f_net_o, f_net_h, eta, hidden_units, alpha, \n",
    "                                                                                        momentum_h, momentum_o, n_classes)\n",
    "            sum_errors += error\n",
    "        epoch += 1\n",
    "        \n",
    "    # Computing the measure for the chosen method\n",
    "    return testing(train, test, hidden_weights, output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = readDataset(\"datasets/wine.csv\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\py36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP(dataset, hidden_layers, hidden_units, n_classes, epochs, eta, alpha, data_ratio):  \n",
    "MLP(dataset, (1,2), 300, 1e-1, 2, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = namedtuple('datset', 'X Y')\n",
    "\n",
    "imagens = pd.read_csv('semeion.data', sep=' ', lineterminator='\\n', header=None).iloc[:,:-1]\n",
    "lenght = imagens.shape[0]\n",
    "\n",
    "imagens = dataset(X=imagens.iloc[:,:256].values, Y=imagens.iloc[:,256:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90996582 0.97109459]\n",
      "[1 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '0.90996582017287430.9710945874749224'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-0c1301839f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-135-685ca945dbd4>\u001b[0m in \u001b[0;36mMLP\u001b[1;34m(dataset, hidden_units, epochs, eta, alpha, train_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m             hidden_weights, output_weights, error, momentum_h, momentum_o = mlp_backward(train, i, hidden_weights, output_weights, \n\u001b[0;32m     50\u001b[0m                                                                                         \u001b[0mf_net_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_net_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                                                                                         momentum_h, momentum_o, n_classes)\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0msum_errors\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-165-ef59b9062ec1>\u001b[0m in \u001b[0;36mmlp_backward\u001b[1;34m(dataset, j, hidden_weights, output_weights, f_net_o, f_net_h, eta, hidden_units, alpha, momentum_h, momentum_o, n_classes)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0my_lista\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mfneto_lista\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf_net_o\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_lista\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfneto_lista\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '0.90996582017287430.9710945874749224'"
     ]
    }
   ],
   "source": [
    "MLP(imagens, (30,), 300, 1e-1, 2, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-147-f67527441ce2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-147-f67527441ce2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    0.join([0,1,0,0])\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "0.join([0,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 1 _ _ _ _ _']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = ['0', '1', '_', '_', '_', '_', '_']\n",
    "result = [' '.join(array)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [0,1,0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
